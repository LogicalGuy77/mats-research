{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3-4B-Thinking-2507 Setup\n",
    "# Run a thinking model locally on RTX 5060 with streaming output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfb498",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4227cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers>=4.51.0\" accelerate bitsandbytes torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b4ab2",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854bb99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "VRAM: 8.08 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5811d",
   "metadata": {},
   "source": [
    "## Step 3: Configure 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc86237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc07a9",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921bb3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded on cuda:0\n",
      "VRAM: 2.67 GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "# Clear any cached models first\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"7GB\"}  # Reserve 1GB for workspace\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded on {model.device}\")\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4617f9e",
   "metadata": {},
   "source": [
    "## Step 5: Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16013671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: Okay, the user is asking about the standard value of acceleration due to gravity on Earth. Let me start by recalling the exact figure. I remember it's 9.8 m/s¬≤, but I should check if there's a more precise value or if it's context-dependent.\n",
      "\n",
      "Hmm, the user might be a student studying physics basics. They probably need this for homework or to understand a concept. But why are they asking? Maybe they encountered different values and are confused. I should clarify that it's an average value since gravity varies by location.\n",
      "\n",
      "Wait, the standard value is often given as 9.80665 m/s¬≤. That's the one used in textbooks for calculations. But in everyday contexts, people say 9.8 m/s¬≤. I should mention both to avoid confusion. \n",
      "\n",
      "Also, the user might not know that gravity changes with altitude, latitude, and local geology. For example, mountains have slightly less gravity than valleys. Including that could add value beyond just the number. \n",
      "\n",
      "I should explain why it's not a constant. The Earth isn't a perfect sphere, and its rotation affects gravity. The equator has less gravity than the poles because of centrifugal force. Maybe the user is wondering why textbooks use a single value if it varies. \n",
      "\n",
      "They might need this for a project where precision matters. If it's for a high school project, 9.8 is sufficient. But if it's engineering, the exact 9.80665 matters. I'll note that the standard value is defined for calculations, but actual measurements differ. \n",
      "\n",
      "Also, units are important. They specified \"acceleration,\" so meters per second squared. Should I mention other units like feet per second squared? Probably not, since the question is about the standard value in SI units. \n",
      "\n",
      "I recall that the International Bureau of Weights and Measures defines g as 9.80665 m/s¬≤ for standard gravity. That's the key point. But I should state it clearly without jargon. \n",
      "\n",
      "Wait, the user might confuse this with the acceleration on other planets. But the question specifies Earth, so I'll focus on that. \n",
      "\n",
      "Let me structure the answer: first state the standard value, explain it's an average, mention variations, and give context on when to use 9.8 vs 9.80665. Also, clarify that it's a defined constant for consistency in science, even though real values differ. \n",
      "\n",
      "Double-checking sources: NASA and standard physics textbooks use 9.8 m/s¬≤ as a rounded value. The precise value is 9.80665. I'll present both but emphasize that 9.8 is common for most purposes. \n",
      "\n",
      "Also, the user might not know about g-force or weight in different locations. But since they asked for acceleration due to gravity, I'll stick to that. \n",
      "\n",
      "Make sure to avoid markdown. Keep it conversational. Start with the standard value, then context, variations, and practical usage. That should cover their needs and potential deeper questions about why it's not constant.\n",
      "</think>\n",
      "\n",
      "CONTENT: The **standard acceleration due to gravity on Earth** is defined as **9.80665 m/s¬≤** (meters per second squared) by the International Bureau of Weights and Measures (BIPM). This value is used as a **reference standard** for scientific calculations and engineering purposes.\n",
      "\n",
      "### Key Context & Clarifications:\n",
      "1. **Why 9.80665?**  \n",
      "   This value is the *average* gravity at sea level at the Earth's equator, adjusted for Earth's oblate shape (it's not a perfect sphere) and rotational effects. It represents the **standard gravity** (symbol: *g‚ÇÄ*) used in physics and metrology for consistency.\n",
      "\n",
      "2. **Common Approximations**  \n",
      "   In most practical scenarios (e.g., high school physics, everyday calculations), it's rounded to:  \n",
      "   - **9.8 m/s¬≤** (most common approximation)  \n",
      "   - **9.81 m/s¬≤** (used in some engineering contexts for greater precision)  \n",
      "   *Example:* If you calculate the weight of an object, you'd use 9.8 m/s¬≤ for simplicity.\n",
      "\n",
      "3. **Why isn't it a constant everywhere?**  \n",
      "   Actual gravity varies slightly based on:  \n",
      "   - **Altitude** (higher altitude = less gravity)  \n",
      "   - **Latitude** (gravity is stronger at the poles than at the equator due to Earth's rotation and oblateness)  \n",
      "   - **Local geology** (e.g., mountains or dense rock deposits affect measurements)  \n",
      "   *Typical real-world values:*  \n",
      "   - At sea level (equator): ~9.780 m/s¬≤  \n",
      "   - At sea level (poles): ~9.832 m/s¬≤  \n",
      "   - In Denver, Colorado (high altitude): ~9.77 m/s¬≤  \n",
      "\n",
      "4. **When to use 9.80665?**  \n",
      "   This exact value is used in:  \n",
      "   - International standards (e.g., calibration of instruments)  \n",
      "   - Theoretical physics calculations where precision matters  \n",
      "   - Defining \"standard gravity\" for units (e.g., the kilogram is defined relative to this value)\n",
      "\n",
      "### Simple Summary for You:\n",
      "| **Context**                     | **Value**       |\n",
      "|----------------------------------|-----------------|\n",
      "| **Exact standard value**        | 9.80665 m/s¬≤    |\n",
      "| **Most common approximation**   | 9.8 m/s¬≤        |\n",
      "| **Real-world variation**        | 9.76‚Äì9.83 m/s¬≤  |\n",
      "\n",
      "**In short:** For 99% of everyday or academic purposes, **9.8 m/s¬≤** is sufficient. But if you need the *defined standard value* (e.g., for precision work or international standards), it's **9.80665 m/s¬≤**.\n",
      "\n",
      "This value ensures consistency across science, even though real gravity varies slightly across Earth's surface. üåç\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=2048):\n",
    "    \"\"\"Generate response with conversation history\"\"\"\n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Apply chat template with full history\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation_history, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    # Parse thinking content (token 151668 is </think>)\n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "    return thinking, content\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"‚úì Conversation history cleared\")\n",
    "\n",
    "def show_history():\n",
    "    \"\"\"Display current conversation history\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation history\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Conversation has {len(conversation_history)} messages:\")\n",
    "    for i, msg in enumerate(conversation_history, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        preview = msg[\"content\"][:60] + \"...\" if len(msg[\"content\"]) > 60 else msg[\"content\"]\n",
    "        print(f\"{i}. [{role}] {preview}\")\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"What is the standard value of acceleration due to gravity on Earth?\"\n",
    "\n",
    "thinking, content = generate_response(prompt)\n",
    "print(\"THINKING:\", thinking)\n",
    "print(\"\\nCONTENT:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1108049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation has 2 messages:\n",
      "1. [USER] What is the standard value of acceleration due to gravity on...\n",
      "2. [ASSISTANT] The **standard acceleration due to gravity on Earth** is def...\n"
     ]
    }
   ],
   "source": [
    "show_history()\n",
    "# clear_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c65067",
   "metadata": {},
   "source": [
    "## Step 6: Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be3b2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: Okay, the user wants me to explain gradient descent in machine learning. Hmm, this is a pretty fundamental concept in ML, so I should make sure I get it right without overwhelming them. \n",
      "\n",
      "First, I wonder about their background. Are they a complete beginner? Maybe a student? Or someone who's heard the term but needs clarification? Since they didn't specify, I'll assume they want a clear but not too technical explanation. \n",
      "\n",
      "I should start with the big picture: why do we even need gradient descent? Because we're optimizing things like loss functions with tons of parameters. Like, imagine having a mountainous landscape and you want to find the lowest point - but you can't see the whole map, you can only take tiny steps. That's the intuition.\n",
      "\n",
      "Wait, I should emphasize it's not about the gradient itself but the direction of steepest descent. People often confuse \"gradient\" with \"slope\" so I should clarify that. Also must mention it's an iterative algorithm - that's crucial. \n",
      "\n",
      "Oh! And the learning rate! That's where beginners get stuck. I should explain why it's called \"learning rate\" and how choosing it wrong breaks everything. Maybe give that example where too big makes it overshoot, too small makes it take forever. \n",
      "\n",
      "...Should I mention variants like SGD vs Adam? Maybe not for the basic explanation. Keep it focused on vanilla gradient descent first. \n",
      "\n",
      "The user might be confused about the math. I'll avoid heavy equations but mention \"partial derivatives\" since that's what the gradient is made of. Like, for each parameter, we compute how much the loss changes with that parameter. \n",
      "\n",
      "*mental note* Must stress that gradient descent finds local minima - not necessarily global minima (unless the function is convex). That's important for real-world models where functions are messy. \n",
      "\n",
      "Hmm, should I include a simple example? Like linear regression? Yeah, that's classic. Show how it updates weights step by step. But don't want to go too deep into the math. \n",
      "\n",
      "*imagines user nodding* They'll probably appreciate the \"why it matters\" part at the end - how this tiny algorithm powers everything from image recognition to chatbots. \n",
      "\n",
      "Wait - is the user preparing for an interview? Might want to know about convergence. But since they didn't ask for advanced details, I'll keep it conceptual. \n",
      "\n",
      "*checks mental list* Okay, structure: \n",
      "1. What it is (simple analogy) \n",
      "2. Why we need it (optimization problem) \n",
      "3. How it works (steps, learning rate) \n",
      "4. Why it's useful (practical impact) \n",
      "\n",
      "Better avoid jargon like \"stochastic\" unless I define it. Save that for if they ask follow-ups. \n",
      "\n",
      "*double-checks* Did I miss anything? Oh! The difference between batch and stochastic gradient descent? Maybe just a footnote since the question is about basic gradient descent. \n",
      "\n",
      "Alright, time to write this clearly. No math unless absolutely necessary. Focus on intuition with concrete examples. That \"hiking down a mountain\" metaphor usually works well.\n",
      "</think>\n",
      "\n",
      "CONTENT: Here's a clear, step-by-step explanation of **gradient descent** in machine learning‚Äîdesigned for beginners but precise enough for intermediate learners. I'll avoid heavy math while covering all essentials.\n",
      "\n",
      "---\n",
      "\n",
      "### üîç **The Core Idea (Intuition First)**\n",
      "Imagine you're hiking in a mountainous area and want to reach the **lowest point** (the global minimum) as quickly as possible. You can't see the whole landscape ahead‚Äîyou only know the **slope** of the ground directly beneath your feet at every step.  \n",
      "**Gradient descent** is your algorithm for this:  \n",
      "> *At each step, you take a small step **down the steepest slope** (toward the lowest point) based on the local terrain you can see.*\n",
      "\n",
      "In ML, this \"lowest point\" is the **optimal model parameters** (e.g., weights in a neural network) that minimize a **loss function** (a measure of how \"wrong\" your model is).\n",
      "\n",
      "---\n",
      "\n",
      "### üß† **Why Do We Need Gradient Descent?**\n",
      "- **The Problem**: ML models have many parameters (e.g., 10,000 weights in a neural net). We need to find the *best* values for these parameters to minimize prediction errors.\n",
      "- **The Challenge**: The loss function (e.g., squared error) is often a complex, curved landscape with many local minima (local \"valleys\"). We can't \"see\" the entire landscape.\n",
      "- **The Solution**: Gradient descent **automatically navigates** this landscape by repeatedly taking tiny steps toward the steepest descent direction.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚öôÔ∏è **How It Actually Works (Step-by-Step)**\n",
      "Let‚Äôs say we have a model with parameter `Œ∏` (e.g., a weight), and a loss function `L(Œ∏)` (e.g., error between predictions and true labels).\n",
      "\n",
      "1. **Start at a random point** (e.g., `Œ∏ = 0.5`).  \n",
      "2. **Compute the gradient** (slope):  \n",
      "   `‚àáL(Œ∏)` = How much the loss changes when we adjust `Œ∏` by a tiny amount.  \n",
      "   *This is the steepest descent direction.*  \n",
      "3. **Take a step**:  \n",
      "   `Œ∏_new = Œ∏ - learning_rate * ‚àáL(Œ∏)`  \n",
      "   - `learning_rate` (denoted as `Œ∑`): A small positive number that controls step size (critical for success!).  \n",
      "4. **Repeat** until the loss stops improving significantly (convergence).\n",
      "\n",
      "---\n",
      "\n",
      "### üåü **Key Concepts Explained Simply**\n",
      "| Concept | What It Is | Why It Matters |\n",
      "|--------|------------|----------------|\n",
      "| **Gradient** | The slope of the loss function at a given point (direction of steepest increase) | Tells you *which way* to move to reduce loss |\n",
      "| **Learning Rate (Œ∑)** | A hyperparameter controlling step size | Too big ‚Üí overshoots minimum (diverges). Too small ‚Üí slow convergence |\n",
      "| **Steepest Descent** | Moving in the direction of the *negative gradient* (downhill) | Ensures maximum progress toward the minimum each step |\n",
      "| **Convergence** | When steps become so small that loss stops improving | The algorithm \"finds\" a minimum (local or global) |\n",
      "\n",
      "> üí° **Real-World Analogy**:  \n",
      "> *Think of gradient descent as a blindfolded hiker using a topographic map (the gradient) to navigate downhill. They can‚Äôt see the whole mountain but know the slope at their current spot‚Äîso they take tiny steps down the steepest path.*\n",
      "\n",
      "---\n",
      "\n",
      "### üö´ **Common Pitfalls & How to Avoid Them**\n",
      "| Issue | Why It Happens | Fix |\n",
      "|-------|----------------|-----|\n",
      "| **Overshooting** (steps too big) | `learning_rate` is too high ‚Üí jumps over the minimum | Reduce `learning_rate` |\n",
      "| **Stuck in local minima** | The landscape has multiple valleys; algorithm gets trapped | Use variants like **Adam** or **RMSprop** (more advanced) |\n",
      "| **No convergence** | Learning rate too low ‚Üí steps are too tiny | Increase `learning_rate` (or use adaptive methods) |\n",
      "\n",
      "---\n",
      "\n",
      "### üí° **Why Is This So Important in ML?**\n",
      "- **It's the foundation of most ML training**: Used in linear regression, neural networks, SVMs, and more.\n",
      "- **Scalable**: Works with millions of parameters (e.g., in image recognition models).\n",
      "- **Efficient**: Finds minima much faster than brute-force search (e.g., trying all possible parameter values).\n",
      "- **Adaptable**: Variants like **Stochastic Gradient Descent (SGD)** and **Adam** handle large datasets efficiently.\n",
      "\n",
      "> ‚úÖ **In a nutshell**: Gradient descent is the \"smartest\" way to automatically find the best model parameters by taking tiny, informed steps downhill in the loss landscape.\n",
      "\n",
      "---\n",
      "\n",
      "### üåü **Example in Practice (Linear Regression)**\n",
      "Suppose we want to fit a line `y = Œ∏‚ÇÄ + Œ∏‚ÇÅx` to data points. The loss is `L = (y_pred - y_true)¬≤`.  \n",
      "- **Step 1**: Start with `Œ∏‚ÇÄ = 0`, `Œ∏‚ÇÅ = 0`.  \n",
      "- **Step 2**: Compute gradient `‚àáL = [‚àÇL/‚àÇŒ∏‚ÇÄ, ‚àÇL/‚àÇŒ∏‚ÇÅ]`.  \n",
      "- **Step 3**: Update: `Œ∏‚ÇÄ_new = Œ∏‚ÇÄ - Œ∑ * ‚àÇL/‚àÇŒ∏‚ÇÄ`, `Œ∏‚ÇÅ_new = Œ∏‚ÇÅ - Œ∑ * ‚àÇL/‚àÇŒ∏‚ÇÅ`.  \n",
      "- **Repeat** until `L` is minimized.\n",
      "\n",
      "---\n",
      "\n",
      "### üíé **Final Takeaway**\n",
      "> **Gradient descent is the algorithm that turns \"I want to minimize error\" into \"Here‚Äôs how to find the best model parameters step by step.\"**  \n",
      "> It‚Äôs not magic‚Äîit‚Äôs a mathematically grounded optimization technique that‚Äôs *practically indispensable* in modern machine learning.\n",
      "\n",
      "If you‚Äôd like to dive deeper into **variations** (like Adam, RMSprop) or **how it handles large datasets**, just ask! üòä\n"
     ]
    }
   ],
   "source": [
    "# Your custom prompt\n",
    "prompt = \"Explain the concept of gradient descent in machine learning.\"\n",
    "\n",
    "thinking, content = generate_response(prompt)\n",
    "print(\"THINKING:\", thinking)\n",
    "print(\"\\nCONTENT:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced71b0e",
   "metadata": {},
   "source": [
    "## Step 7: Monitor VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cefdfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM: 2.68 / 8.08 GB\n"
     ]
    }
   ],
   "source": [
    "allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"VRAM: {allocated:.2f} / {total:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ff76e",
   "metadata": {},
   "source": [
    "## Phase 0: Manual Interpretability Setup & Validation\n",
    "\n",
    "Testing manual interpretability tools under VRAM constraints. This validates that we can extract and analyze hidden states for all downstream experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79aafc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up manual interpretability tools...\n",
      "‚úì Using 4-bit quantized model for all analysis\n",
      "‚úì Manual approach: output_hidden_states=True + PyTorch hooks\n",
      "\n",
      "‚úì Manual interpretability setup complete!\n",
      "VRAM: 2.68 GB\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Manual Interpretability Setup\n",
    "# Using manual PyTorch hooks for full control and VRAM efficiency\n",
    "\n",
    "print(\"Setting up manual interpretability tools...\")\n",
    "print(\"‚úì Using 4-bit quantized model for all analysis\")\n",
    "print(\"‚úì Manual approach: output_hidden_states=True + PyTorch hooks\")\n",
    "\n",
    "# Tools available:\n",
    "# 1. model.model.layers - access to all 36 transformer layers\n",
    "# 2. output_hidden_states=True - gets activations at every layer\n",
    "# 3. PyTorch hooks - for activation patching\n",
    "# 4. model.lm_head + model.model.norm - for Logit Lens decoding\n",
    "\n",
    "print(\"\\n‚úì Manual interpretability setup complete!\")\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895869a",
   "metadata": {},
   "source": [
    "### Validate Hidden State Access\n",
    "\n",
    "Test that we can extract internal activations from all 36 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "331754d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting hidden states from quantized model...\n",
      "Model architecture: qwen3\n",
      "Number of layers: 36\n",
      "\n",
      "‚úì Hidden State Validation:\n",
      "  ‚Ä¢ Hidden states accessible: 37 tensors\n",
      "  ‚Ä¢ (Index 0 = embeddings, indices 1-36 = layer outputs)\n",
      "  ‚Ä¢ Shape per layer: torch.Size([1, 8, 2560])\n",
      "  ‚Ä¢ Input tokens: 8\n",
      "\n",
      "‚úì Model's next token prediction: ' '\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "test_text = \"The acceleration due to gravity on Earth is\"\n",
    "\n",
    "# Use manual activation extraction\n",
    "print(\"Extracting hidden states from quantized model...\")\n",
    "print(f\"Model architecture: {model.config.model_type}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "\n",
    "# Run inference with hidden state extraction\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "print(f\"\\n‚úì Hidden State Validation:\")\n",
    "print(f\"  ‚Ä¢ Hidden states accessible: {len(outputs.hidden_states)} tensors\")\n",
    "print(f\"  ‚Ä¢ (Index 0 = embeddings, indices 1-36 = layer outputs)\")\n",
    "print(f\"  ‚Ä¢ Shape per layer: {outputs.hidden_states[0].shape}\")\n",
    "print(f\"  ‚Ä¢ Input tokens: {inputs.input_ids.shape[1]}\")\n",
    "\n",
    "predicted_token_id = outputs.logits[0, -1].argmax()\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(f\"\\n‚úì Model's next token prediction: '{predicted_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4012a9",
   "metadata": {},
   "source": [
    "### Phase 0 Complete ‚úì\n",
    "\n",
    "**Manual Interpretability Setup**\n",
    "\n",
    "You now have the core tools validated:\n",
    "- ‚úì **Hidden State Extraction**: `output_hidden_states=True` captures all 36 layers\n",
    "- ‚úì **Logit Lens**: `model.lm_head(model.model.norm(hidden_state))` decodes any layer\n",
    "- ‚úì **Activation Patching**: PyTorch hooks for causal interventions\n",
    "- ‚úì **VRAM Efficient**: Single 4-bit model (~3GB VRAM)\n",
    "\n",
    "Ready for Phase 1: Testing CoT faithfulness with false beliefs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

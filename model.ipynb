{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "924556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3-4B-Thinking-2507 Setup\n",
    "# Run a thinking model locally on RTX 5060 with streaming output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfb498",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4227cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers>=4.51.0\" accelerate bitsandbytes torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b4ab2",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854bb99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "VRAM: 8.08 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5811d",
   "metadata": {},
   "source": [
    "## Step 3: Configure 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc86237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc07a9",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "921bb3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.36s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded on cuda:0\n",
      "VRAM: 4.58 GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model loaded on {model.device}\")\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4617f9e",
   "metadata": {},
   "source": [
    "## Step 5: Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16013671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: Okay, let's see. I need to find the average speed for the entire journey of the train. Hmm, average speed is total distance divided by total time, right? So first, I should figure out the total distance the train traveled and the total time it took, including the stop. \n",
      "\n",
      "Let me break it down step by step. \n",
      "\n",
      "First part: The train travels 120 km in 2 hours. So that's distance 1: 120 km, time 1: 2 hours.\n",
      "\n",
      "Then it stops for 30 minutes. Wait, 30 minutes is 0.5 hours, right? So that's a stop time of 0.5 hours.\n",
      "\n",
      "Next part: It travels another 90 km in 1.5 hours. So distance 2: 90 km, time 2: 1.5 hours.\n",
      "\n",
      "So total distance would be 120 km + 90 km. Let me calculate that: 120 + 90 = 210 km. Got that.\n",
      "\n",
      "Total time is the sum of all the time segments. So time for first part: 2 hours, stop time: 0.5 hours, time for second part: 1.5 hours. So total time = 2 + 0.5 + 1.5. Let me add that up. 2 + 0.5 is 2.5, plus 1.5 is 4 hours. So total time is 4 hours.\n",
      "\n",
      "Wait, average speed is total distance over total time. So 210 km divided by 4 hours. Let me compute that. 210 divided by 4. Let's see, 4 times 50 is 200, so 210 is 10 more, so 50 + 10/4 = 50 + 2.5 = 52.5 km/h. \n",
      "\n",
      "Wait, but let me double-check. Sometimes people confuse average speed with average velocity, but here it's speed, so it's just total distance over total time, including stops. Because average speed is total distance traveled divided by total time taken, including stops. So yes, the stop time is part of the total time. \n",
      "\n",
      "Let me confirm the total time again. First part: 2 hours. Stop: 30 minutes = 0.5 hours. Second part: 1.5 hours. So 2 + 0.5 + 1.5 = 4 hours. Correct.\n",
      "\n",
      "Total distance: 120 + 90 = 210 km. Correct.\n",
      "\n",
      "So average speed is 210 km / 4 h = 52.5 km/h. \n",
      "\n",
      "I think that's it. Let me make sure I didn't miss anything. The key point is that the stop time is included in the total time, which is important because sometimes people might forget that. If you forgot the stop time, you would get 210 km / (2 + 1.5) = 210 / 3.5 = 60 km/h, but that's incorrect because the stop time is part of the journey time. So including the 0.5 hours is crucial here.\n",
      "\n",
      "Yes, so the correct average speed is 52.5 km/h.\n",
      "</think>\n",
      "\n",
      "CONTENT: To determine the **average speed** for the entire journey, we follow a step-by-step approach based on the definition of average speed:\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{\\text{Total Distance Traveled}}{\\text{Total Time Taken (including stops)}}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Calculate Total Distance Traveled**\n",
      "\n",
      "The train travels:\n",
      "- First segment: 120 km\n",
      "- Second segment: 90 km\n",
      "\n",
      "$$\n",
      "\\text{Total Distance} = 120 \\, \\text{km} + 90 \\, \\text{km} = 210 \\, \\text{km}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 2: Calculate Total Time Taken**\n",
      "\n",
      "We need to include **all** time segments:\n",
      "- Time for first segment: 2 hours\n",
      "- Stop time: 30 minutes = $ \\frac{30}{60} = 0.5 $ hours\n",
      "- Time for second segment: 1.5 hours\n",
      "\n",
      "$$\n",
      "\\text{Total Time} = 2 + 0.5 + 1.5 = 4 \\, \\text{hours}\n",
      "$$\n",
      "\n",
      "Note: The stop time is included in the total time because it is part of the overall journey duration.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 3: Compute Average Speed**\n",
      "\n",
      "$$\n",
      "\\text{Average Speed} = \\frac{210 \\, \\text{km}}{4 \\, \\text{hours}} = 52.5 \\, \\text{km/h}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Final Answer:\n",
      "\n",
      "$$\n",
      "\\boxed{52.5} \\, \\text{km/h}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, max_new_tokens=2048):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    # Parse thinking content (token 151668 is </think>)\n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    return thinking, content\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"Solve this step by step: If a train travels 120 km in 2 hours, then stops for 30 minutes, then travels another 90 km in 1.5 hours, what is the average speed for the entire journey?\"\n",
    "\n",
    "thinking, content = generate_response(prompt)\n",
    "print(\"THINKING:\", thinking)\n",
    "print(\"\\nCONTENT:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c65067",
   "metadata": {},
   "source": [
    "## Step 6: Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be3b2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: Okay, the user wants me to explain gradient descent in machine learning. Hmm, this is a pretty fundamental concept in ML, so I should make sure I get it right without overwhelming them. \n",
      "\n",
      "First, I wonder about their background. Are they a complete beginner? Maybe a student? Or someone who's heard the term but needs clarification? Since they didn't specify, I'll assume they want a clear but not too technical explanation. \n",
      "\n",
      "I should start with the big picture: why do we even need gradient descent? Because we're optimizing things like loss functions with tons of parameters. Like, imagine having a mountainous landscape and you want to find the lowest point - but you can't see the whole map, you can only take tiny steps. That's the intuition.\n",
      "\n",
      "Wait, I should emphasize it's not about the gradient itself but the direction of steepest descent. People often confuse \"gradient\" with \"slope\" so I should clarify that. Also must mention it's an iterative algorithm - that's crucial. \n",
      "\n",
      "Oh! And the learning rate! That's where beginners get stuck. I should explain why it's called \"learning rate\" and how choosing it wrong breaks everything. Maybe give that example where too big makes it overshoot, too small makes it take forever. \n",
      "\n",
      "...Should I mention variants like SGD vs Adam? Maybe not for the basic explanation. Keep it focused on vanilla gradient descent first. \n",
      "\n",
      "The user might be confused about the math. I'll avoid heavy equations but mention \"partial derivatives\" since that's what the gradient is made of. Like, for each parameter, we compute how much the loss changes with that parameter. \n",
      "\n",
      "*mental note* Must stress that gradient descent finds local minima - not necessarily global minima (unless the function is convex). That's important for real-world models where functions are messy. \n",
      "\n",
      "Hmm, should I include a simple example? Like linear regression? Yeah, that's classic. Show how it updates weights step by step. But don't want to go too deep into the math. \n",
      "\n",
      "*imagines user nodding* They'll probably appreciate the \"why it matters\" part at the end - how this tiny algorithm powers everything from image recognition to chatbots. \n",
      "\n",
      "Wait - is the user preparing for an interview? Might want to know about convergence. But since they didn't ask for advanced details, I'll keep it conceptual. \n",
      "\n",
      "*checks mental list* Okay, structure: \n",
      "1. What it is (simple analogy) \n",
      "2. Why we need it (optimization problem) \n",
      "3. How it works (steps, learning rate) \n",
      "4. Why it's useful (practical impact) \n",
      "\n",
      "Better avoid jargon like \"stochastic\" unless I define it. Save that for if they ask follow-ups. \n",
      "\n",
      "*double-checks* Did I miss anything? Oh! The difference between batch and stochastic gradient descent? Maybe just a footnote since the question is about basic gradient descent. \n",
      "\n",
      "Alright, time to write this clearly. No math unless absolutely necessary. Focus on intuition with concrete examples. That \"hiking down a mountain\" metaphor usually works well.\n",
      "</think>\n",
      "\n",
      "CONTENT: Here's a clear, step-by-step explanation of **gradient descent** in machine learningâ€”designed for beginners but precise enough for intermediate learners. I'll avoid heavy math while covering all essentials.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ” **The Core Idea (Intuition First)**\n",
      "Imagine you're hiking in a mountainous area and want to reach the **lowest point** (the global minimum) as quickly as possible. You can't see the whole landscape aheadâ€”you only know the **slope** of the ground directly beneath your feet at every step.  \n",
      "**Gradient descent** is your algorithm for this:  \n",
      "> *At each step, you take a small step **down the steepest slope** (toward the lowest point) based on the local terrain you can see.*\n",
      "\n",
      "In ML, this \"lowest point\" is the **optimal model parameters** (e.g., weights in a neural network) that minimize a **loss function** (a measure of how \"wrong\" your model is).\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ§  **Why Do We Need Gradient Descent?**\n",
      "- **The Problem**: ML models have many parameters (e.g., 10,000 weights in a neural net). We need to find the *best* values for these parameters to minimize prediction errors.\n",
      "- **The Challenge**: The loss function (e.g., squared error) is often a complex, curved landscape with many local minima (local \"valleys\"). We can't \"see\" the entire landscape.\n",
      "- **The Solution**: Gradient descent **automatically navigates** this landscape by repeatedly taking tiny steps toward the steepest descent direction.\n",
      "\n",
      "---\n",
      "\n",
      "### âš™ï¸ **How It Actually Works (Step-by-Step)**\n",
      "Letâ€™s say we have a model with parameter `Î¸` (e.g., a weight), and a loss function `L(Î¸)` (e.g., error between predictions and true labels).\n",
      "\n",
      "1. **Start at a random point** (e.g., `Î¸ = 0.5`).  \n",
      "2. **Compute the gradient** (slope):  \n",
      "   `âˆ‡L(Î¸)` = How much the loss changes when we adjust `Î¸` by a tiny amount.  \n",
      "   *This is the steepest descent direction.*  \n",
      "3. **Take a step**:  \n",
      "   `Î¸_new = Î¸ - learning_rate * âˆ‡L(Î¸)`  \n",
      "   - `learning_rate` (denoted as `Î·`): A small positive number that controls step size (critical for success!).  \n",
      "4. **Repeat** until the loss stops improving significantly (convergence).\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸŒŸ **Key Concepts Explained Simply**\n",
      "| Concept | What It Is | Why It Matters |\n",
      "|--------|------------|----------------|\n",
      "| **Gradient** | The slope of the loss function at a given point (direction of steepest increase) | Tells you *which way* to move to reduce loss |\n",
      "| **Learning Rate (Î·)** | A hyperparameter controlling step size | Too big â†’ overshoots minimum (diverges). Too small â†’ slow convergence |\n",
      "| **Steepest Descent** | Moving in the direction of the *negative gradient* (downhill) | Ensures maximum progress toward the minimum each step |\n",
      "| **Convergence** | When steps become so small that loss stops improving | The algorithm \"finds\" a minimum (local or global) |\n",
      "\n",
      "> ðŸ’¡ **Real-World Analogy**:  \n",
      "> *Think of gradient descent as a blindfolded hiker using a topographic map (the gradient) to navigate downhill. They canâ€™t see the whole mountain but know the slope at their current spotâ€”so they take tiny steps down the steepest path.*\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸš« **Common Pitfalls & How to Avoid Them**\n",
      "| Issue | Why It Happens | Fix |\n",
      "|-------|----------------|-----|\n",
      "| **Overshooting** (steps too big) | `learning_rate` is too high â†’ jumps over the minimum | Reduce `learning_rate` |\n",
      "| **Stuck in local minima** | The landscape has multiple valleys; algorithm gets trapped | Use variants like **Adam** or **RMSprop** (more advanced) |\n",
      "| **No convergence** | Learning rate too low â†’ steps are too tiny | Increase `learning_rate` (or use adaptive methods) |\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ’¡ **Why Is This So Important in ML?**\n",
      "- **It's the foundation of most ML training**: Used in linear regression, neural networks, SVMs, and more.\n",
      "- **Scalable**: Works with millions of parameters (e.g., in image recognition models).\n",
      "- **Efficient**: Finds minima much faster than brute-force search (e.g., trying all possible parameter values).\n",
      "- **Adaptable**: Variants like **Stochastic Gradient Descent (SGD)** and **Adam** handle large datasets efficiently.\n",
      "\n",
      "> âœ… **In a nutshell**: Gradient descent is the \"smartest\" way to automatically find the best model parameters by taking tiny, informed steps downhill in the loss landscape.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸŒŸ **Example in Practice (Linear Regression)**\n",
      "Suppose we want to fit a line `y = Î¸â‚€ + Î¸â‚x` to data points. The loss is `L = (y_pred - y_true)Â²`.  \n",
      "- **Step 1**: Start with `Î¸â‚€ = 0`, `Î¸â‚ = 0`.  \n",
      "- **Step 2**: Compute gradient `âˆ‡L = [âˆ‚L/âˆ‚Î¸â‚€, âˆ‚L/âˆ‚Î¸â‚]`.  \n",
      "- **Step 3**: Update: `Î¸â‚€_new = Î¸â‚€ - Î· * âˆ‚L/âˆ‚Î¸â‚€`, `Î¸â‚_new = Î¸â‚ - Î· * âˆ‚L/âˆ‚Î¸â‚`.  \n",
      "- **Repeat** until `L` is minimized.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ’Ž **Final Takeaway**\n",
      "> **Gradient descent is the algorithm that turns \"I want to minimize error\" into \"Hereâ€™s how to find the best model parameters step by step.\"**  \n",
      "> Itâ€™s not magicâ€”itâ€™s a mathematically grounded optimization technique thatâ€™s *practically indispensable* in modern machine learning.\n",
      "\n",
      "If youâ€™d like to dive deeper into **variations** (like Adam, RMSprop) or **how it handles large datasets**, just ask! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Your custom prompt\n",
    "prompt = \"Explain the concept of gradient descent in machine learning.\"\n",
    "\n",
    "thinking, content = generate_response(prompt)\n",
    "print(\"THINKING:\", thinking)\n",
    "print(\"\\nCONTENT:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced71b0e",
   "metadata": {},
   "source": [
    "## Step 7: Monitor VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cefdfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM: 4.58 / 8.08 GB\n"
     ]
    }
   ],
   "source": [
    "allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"VRAM: {allocated:.2f} / {total:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

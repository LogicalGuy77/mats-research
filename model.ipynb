{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "924556c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3-4B-Thinking-2507 Setup\n",
    "# Run a thinking model locally on RTX 5060 with streaming output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfb498",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4227cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers>=4.51.0\" accelerate bitsandbytes torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b4ab2",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854bb99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "VRAM: 8.08 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5811d",
   "metadata": {},
   "source": [
    "## Step 3: Configure 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc86237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc07a9",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "921bb3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.36s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded on cuda:0\n",
      "VRAM: 4.58 GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded on {model.device}\")\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4617f9e",
   "metadata": {},
   "source": [
    "## Step 5: Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16013671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: Okay, the user is asking about the standard value of acceleration due to gravity on Earth. Let me start by recalling the exact figure. I remember it's 9.8 m/s¬≤, but I should double-check if there's a more precise value or if it varies.\n",
      "\n",
      "Hmm, the user might be a student studying physics basics. They probably need this for homework or to understand fundamental concepts. But why are they asking? Maybe they encountered different values and want clarification. I should explain why it's not a fixed number everywhere.\n",
      "\n",
      "Wait, the standard gravity is defined as 9.80665 m/s¬≤. But in many textbooks, they round it to 9.8 m/s¬≤. I should mention both to avoid confusion. Also, the user might not know that gravity varies by location‚Äîlike mountains vs. valleys, or the equator vs. poles. \n",
      "\n",
      "I should note that the variation is due to Earth's shape and rotation. The equator has less gravity because of the centrifugal force and Earth's bulge. The poles have more. But the standard value is an average. \n",
      "\n",
      "The user might also be confused between \"standard gravity\" and actual measurements. Like, in labs, they might use 9.81 m/s¬≤. I should clarify the difference between the defined standard and real-world values. \n",
      "\n",
      "Also, units‚Äîmake sure to specify meters per second squared. Sometimes people mix up units. And emphasize that it's an average value. \n",
      "\n",
      "Wait, the user's deeper need might be understanding why different sources give slightly different numbers. So explaining the context of standard vs. local gravity is important. They might need this for a project where precision matters, like engineering, but for most purposes, 9.8 is sufficient. \n",
      "\n",
      "I should structure the answer: first state the standard value, then mention variations, and why it's an average. Highlight common approximations too. Avoid jargon but be precise. \n",
      "\n",
      "Check if there's a chance they're mixing it with other contexts, like gravity on other planets. But the question specifies Earth, so stick to that. \n",
      "\n",
      "Also, confirm the exact value from reliable sources. I recall the International Gravity Formula uses 9.780327 m/s¬≤ at sea level, but the standard gravity is 9.80665. Need to be careful not to confuse the two. \n",
      "\n",
      "Yeah, the key points are: standard value is 9.80665 m/s¬≤, commonly rounded to 9.8 or 9.81, and it varies by location. Make sure to mention that it's a defined constant for measurements, not the actual gravity at a specific spot. \n",
      "\n",
      "The user might benefit from a quick example: why is it 9.80665? Because it's the acceleration at sea level at 45 degrees latitude, but actually, the standard gravity is a defined value for consistency in science, not a specific location. \n",
      "\n",
      "I think I have it. Start with the exact standard value, then practical approximations, and the factors causing variation. Keep it clear and concise.\n",
      "</think>\n",
      "\n",
      "CONTENT: The **standard value of acceleration due to gravity on Earth** is defined as:  \n",
      "**9.80665 m/s¬≤** (meters per second squared).  \n",
      "\n",
      "This value is **exactly defined** by the International Bureau of Weights and Measures (BIPM) as the **\"standard gravity\"** (denoted as *g‚ÇÄ*). It is used as a reference standard for scientific calculations, engineering, and education where a consistent baseline is needed, *regardless of actual local gravity variations*.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Details & Context:\n",
      "1. **Why 9.80665 m/s¬≤?**  \n",
      "   - This value represents the **average acceleration due to gravity at sea level** at **45¬∞ latitude** (a specific point chosen for standardization).  \n",
      "   - It accounts for Earth's shape, rotation, and density variations to create a consistent reference.\n",
      "\n",
      "2. **Common Approximations (for practical use):**  \n",
      "   - **9.8 m/s¬≤** (most common in introductory physics).  \n",
      "   - **9.81 m/s¬≤** (used in engineering and some textbooks for better precision).  \n",
      "   *Example:* If a problem states \"g = 10 m/s¬≤\" for simplicity, it's an approximation (not the standard value).\n",
      "\n",
      "3. **Why does gravity vary *locally*?**  \n",
      "   Actual gravity at any specific location on Earth **differs from 9.80665 m/s¬≤** due to:  \n",
      "   - Earth's **oblate shape** (bulging at the equator ‚Üí weaker gravity at the equator).  \n",
      "   - **Altitude** (higher elevations ‚Üí lower gravity).  \n",
      "   - **Local geology** (dense rock formations ‚Üí stronger gravity; less dense areas ‚Üí weaker).  \n",
      "   - **Earth's rotation** (centrifugal force reduces effective gravity at the equator by ~0.3%).  \n",
      "   - *Actual range*: From **~9.76 m/s¬≤** (e.g., on the equator at sea level) to **~9.83 m/s¬≤** (e.g., at the poles).\n",
      "\n",
      "4. **Real-world vs. Standard Value**:  \n",
      "   - **In labs or engineering**: If you measure gravity at your location, it will be close to 9.8 m/s¬≤ but *not exactly* 9.80665.  \n",
      "   - **For calculations**: Scientists use **9.80665 m/s¬≤** as a universal constant (like \"1 meter = 1 m\") to avoid confusion.  \n",
      "   - **In space or other planets**: Gravity is different (e.g., Moon: ~1.6 m/s¬≤).\n",
      "\n",
      "---\n",
      "\n",
      "### Summary Table:\n",
      "| Context                     | Value                     | Notes                                  |\n",
      "|-----------------------------|----------------------------|-----------------------------------------|\n",
      "| **Standard gravity** (defined) | **9.80665 m/s¬≤**         | Universal reference value              |\n",
      "| **Common approximation**    | 9.8 m/s¬≤                  | Introductory physics (most common)     |\n",
      "| **Engineering use**         | 9.81 m/s¬≤                 | Better accuracy for calculations       |\n",
      "| **Actual gravity at sea level** | 9.78‚Äì9.83 m/s¬≤       | Varies by location (e.g., equator vs. poles) |\n",
      "\n",
      "This standard value ensures consistency across global scientific work, even though real-world gravity fluctuates slightly. For most everyday purposes (e.g., school problems), **9.8 m/s¬≤** is perfectly adequate. üåç\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=2048):\n",
    "    \"\"\"Generate response with conversation history\"\"\"\n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Apply chat template with full history\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation_history, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    # Parse thinking content (token 151668 is </think>)\n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "    return thinking, content\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"‚úì Conversation history cleared\")\n",
    "\n",
    "def show_history():\n",
    "    \"\"\"Display current conversation history\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation history\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Conversation has {len(conversation_history)} messages:\")\n",
    "    for i, msg in enumerate(conversation_history, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        preview = msg[\"content\"][:60] + \"...\" if len(msg[\"content\"]) > 60 else msg[\"content\"]\n",
    "        print(f\"{i}. [{role}] {preview}\")\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"What is the standard value of acceleration due to gravity on Earth?\"\n",
    "\n",
    "thinking, content = generate_response(prompt)\n",
    "print(\"THINKING:\", thinking)\n",
    "print(\"\\nCONTENT:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1108049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation has 2 messages:\n",
      "1. [USER] What is the standard value of acceleration due to gravity on...\n",
      "2. [ASSISTANT] The **standard value of acceleration due to gravity on Earth...\n"
     ]
    }
   ],
   "source": [
    "show_history()\n",
    "# clear_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c65067",
   "metadata": {},
   "source": [
    "## Step 6: Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be3b2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THINKING: Okay, the user wants me to explain gradient descent in machine learning. Hmm, this is a pretty fundamental concept in ML, so I should make sure I get it right without overwhelming them. \n",
      "\n",
      "First, I wonder about their background. Are they a complete beginner? Maybe a student? Or someone who's heard the term but needs clarification? Since they didn't specify, I'll assume they want a clear but not too technical explanation. \n",
      "\n",
      "I should start with the big picture: why do we even need gradient descent? Because we're optimizing things like loss functions with tons of parameters. Like, imagine having a mountainous landscape and you want to find the lowest point - but you can't see the whole map, you can only take tiny steps. That's the intuition.\n",
      "\n",
      "Wait, I should emphasize it's not about the gradient itself but the direction of steepest descent. People often confuse \"gradient\" with \"slope\" so I should clarify that. Also must mention it's an iterative algorithm - that's crucial. \n",
      "\n",
      "Oh! And the learning rate! That's where beginners get stuck. I should explain why it's called \"learning rate\" and how choosing it wrong breaks everything. Maybe give that example where too big makes it overshoot, too small makes it take forever. \n",
      "\n",
      "...Should I mention variants like SGD vs Adam? Maybe not for the basic explanation. Keep it focused on vanilla gradient descent first. \n",
      "\n",
      "The user might be confused about the math. I'll avoid heavy equations but mention \"partial derivatives\" since that's what the gradient is made of. Like, for each parameter, we compute how much the loss changes with that parameter. \n",
      "\n",
      "*mental note* Must stress that gradient descent finds local minima - not necessarily global minima (unless the function is convex). That's important for real-world models where functions are messy. \n",
      "\n",
      "Hmm, should I include a simple example? Like linear regression? Yeah, that's classic. Show how it updates weights step by step. But don't want to go too deep into the math. \n",
      "\n",
      "*imagines user nodding* They'll probably appreciate the \"why it matters\" part at the end - how this tiny algorithm powers everything from image recognition to chatbots. \n",
      "\n",
      "Wait - is the user preparing for an interview? Might want to know about convergence. But since they didn't ask for advanced details, I'll keep it conceptual. \n",
      "\n",
      "*checks mental list* Okay, structure: \n",
      "1. What it is (simple analogy) \n",
      "2. Why we need it (optimization problem) \n",
      "3. How it works (steps, learning rate) \n",
      "4. Why it's useful (practical impact) \n",
      "\n",
      "Better avoid jargon like \"stochastic\" unless I define it. Save that for if they ask follow-ups. \n",
      "\n",
      "*double-checks* Did I miss anything? Oh! The difference between batch and stochastic gradient descent? Maybe just a footnote since the question is about basic gradient descent. \n",
      "\n",
      "Alright, time to write this clearly. No math unless absolutely necessary. Focus on intuition with concrete examples. That \"hiking down a mountain\" metaphor usually works well.\n",
      "</think>\n",
      "\n",
      "CONTENT: Here's a clear, step-by-step explanation of **gradient descent** in machine learning‚Äîdesigned for beginners but precise enough for intermediate learners. I'll avoid heavy math while covering all essentials.\n",
      "\n",
      "---\n",
      "\n",
      "### üîç **The Core Idea (Intuition First)**\n",
      "Imagine you're hiking in a mountainous area and want to reach the **lowest point** (the global minimum) as quickly as possible. You can't see the whole landscape ahead‚Äîyou only know the **slope** of the ground directly beneath your feet at every step.  \n",
      "**Gradient descent** is your algorithm for this:  \n",
      "> *At each step, you take a small step **down the steepest slope** (toward the lowest point) based on the local terrain you can see.*\n",
      "\n",
      "In ML, this \"lowest point\" is the **optimal model parameters** (e.g., weights in a neural network) that minimize a **loss function** (a measure of how \"wrong\" your model is).\n",
      "\n",
      "---\n",
      "\n",
      "### üß† **Why Do We Need Gradient Descent?**\n",
      "- **The Problem**: ML models have many parameters (e.g., 10,000 weights in a neural net). We need to find the *best* values for these parameters to minimize prediction errors.\n",
      "- **The Challenge**: The loss function (e.g., squared error) is often a complex, curved landscape with many local minima (local \"valleys\"). We can't \"see\" the entire landscape.\n",
      "- **The Solution**: Gradient descent **automatically navigates** this landscape by repeatedly taking tiny steps toward the steepest descent direction.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚öôÔ∏è **How It Actually Works (Step-by-Step)**\n",
      "Let‚Äôs say we have a model with parameter `Œ∏` (e.g., a weight), and a loss function `L(Œ∏)` (e.g., error between predictions and true labels).\n",
      "\n",
      "1. **Start at a random point** (e.g., `Œ∏ = 0.5`).  \n",
      "2. **Compute the gradient** (slope):  \n",
      "   `‚àáL(Œ∏)` = How much the loss changes when we adjust `Œ∏` by a tiny amount.  \n",
      "   *This is the steepest descent direction.*  \n",
      "3. **Take a step**:  \n",
      "   `Œ∏_new = Œ∏ - learning_rate * ‚àáL(Œ∏)`  \n",
      "   - `learning_rate` (denoted as `Œ∑`): A small positive number that controls step size (critical for success!).  \n",
      "4. **Repeat** until the loss stops improving significantly (convergence).\n",
      "\n",
      "---\n",
      "\n",
      "### üåü **Key Concepts Explained Simply**\n",
      "| Concept | What It Is | Why It Matters |\n",
      "|--------|------------|----------------|\n",
      "| **Gradient** | The slope of the loss function at a given point (direction of steepest increase) | Tells you *which way* to move to reduce loss |\n",
      "| **Learning Rate (Œ∑)** | A hyperparameter controlling step size | Too big ‚Üí overshoots minimum (diverges). Too small ‚Üí slow convergence |\n",
      "| **Steepest Descent** | Moving in the direction of the *negative gradient* (downhill) | Ensures maximum progress toward the minimum each step |\n",
      "| **Convergence** | When steps become so small that loss stops improving | The algorithm \"finds\" a minimum (local or global) |\n",
      "\n",
      "> üí° **Real-World Analogy**:  \n",
      "> *Think of gradient descent as a blindfolded hiker using a topographic map (the gradient) to navigate downhill. They can‚Äôt see the whole mountain but know the slope at their current spot‚Äîso they take tiny steps down the steepest path.*\n",
      "\n",
      "---\n",
      "\n",
      "### üö´ **Common Pitfalls & How to Avoid Them**\n",
      "| Issue | Why It Happens | Fix |\n",
      "|-------|----------------|-----|\n",
      "| **Overshooting** (steps too big) | `learning_rate` is too high ‚Üí jumps over the minimum | Reduce `learning_rate` |\n",
      "| **Stuck in local minima** | The landscape has multiple valleys; algorithm gets trapped | Use variants like **Adam** or **RMSprop** (more advanced) |\n",
      "| **No convergence** | Learning rate too low ‚Üí steps are too tiny | Increase `learning_rate` (or use adaptive methods) |\n",
      "\n",
      "---\n",
      "\n",
      "### üí° **Why Is This So Important in ML?**\n",
      "- **It's the foundation of most ML training**: Used in linear regression, neural networks, SVMs, and more.\n",
      "- **Scalable**: Works with millions of parameters (e.g., in image recognition models).\n",
      "- **Efficient**: Finds minima much faster than brute-force search (e.g., trying all possible parameter values).\n",
      "- **Adaptable**: Variants like **Stochastic Gradient Descent (SGD)** and **Adam** handle large datasets efficiently.\n",
      "\n",
      "> ‚úÖ **In a nutshell**: Gradient descent is the \"smartest\" way to automatically find the best model parameters by taking tiny, informed steps downhill in the loss landscape.\n",
      "\n",
      "---\n",
      "\n",
      "### üåü **Example in Practice (Linear Regression)**\n",
      "Suppose we want to fit a line `y = Œ∏‚ÇÄ + Œ∏‚ÇÅx` to data points. The loss is `L = (y_pred - y_true)¬≤`.  \n",
      "- **Step 1**: Start with `Œ∏‚ÇÄ = 0`, `Œ∏‚ÇÅ = 0`.  \n",
      "- **Step 2**: Compute gradient `‚àáL = [‚àÇL/‚àÇŒ∏‚ÇÄ, ‚àÇL/‚àÇŒ∏‚ÇÅ]`.  \n",
      "- **Step 3**: Update: `Œ∏‚ÇÄ_new = Œ∏‚ÇÄ - Œ∑ * ‚àÇL/‚àÇŒ∏‚ÇÄ`, `Œ∏‚ÇÅ_new = Œ∏‚ÇÅ - Œ∑ * ‚àÇL/‚àÇŒ∏‚ÇÅ`.  \n",
      "- **Repeat** until `L` is minimized.\n",
      "\n",
      "---\n",
      "\n",
      "### üíé **Final Takeaway**\n",
      "> **Gradient descent is the algorithm that turns \"I want to minimize error\" into \"Here‚Äôs how to find the best model parameters step by step.\"**  \n",
      "> It‚Äôs not magic‚Äîit‚Äôs a mathematically grounded optimization technique that‚Äôs *practically indispensable* in modern machine learning.\n",
      "\n",
      "If you‚Äôd like to dive deeper into **variations** (like Adam, RMSprop) or **how it handles large datasets**, just ask! üòä\n"
     ]
    }
   ],
   "source": [
    "# Your custom prompt\n",
    "prompt = \"Explain the concept of gradient descent in machine learning.\"\n",
    "\n",
    "thinking, content = generate_response(prompt)\n",
    "print(\"THINKING:\", thinking)\n",
    "print(\"\\nCONTENT:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced71b0e",
   "metadata": {},
   "source": [
    "## Step 7: Monitor VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9cefdfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM: 2.74 / 8.08 GB\n"
     ]
    }
   ],
   "source": [
    "allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"VRAM: {allocated:.2f} / {total:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ff76e",
   "metadata": {},
   "source": [
    "## Phase 0: TransformerLens Setup & Validation\n",
    "\n",
    "Testing interpretability tooling under VRAM constraints. This is **non-negotiable** - if hooks fail, all downstream mech interp claims are invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79aafc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up manual interpretability tools...\n",
      "‚úì Using existing 4-bit quantized model for all analysis\n",
      "‚úì This approach is used by many mech interp researchers\n",
      "\n",
      "‚úì Manual interpretability setup complete!\n",
      "VRAM: 2.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Phase 0: Manual Interpretability Setup\n",
    "# TransformerLens doesn't support Qwen3-4B-Thinking yet, so we'll use manual hooks\n",
    "# This is actually better for VRAM constraints and gives you full control!\n",
    "\n",
    "print(\"Setting up manual interpretability tools...\")\n",
    "print(\"‚úì Using existing 4-bit quantized model for all analysis\")\n",
    "print(\"‚úì This approach is used by many mech interp researchers\")\n",
    "\n",
    "# We already have everything we need:\n",
    "# 1. model.model.layers - access to all transformer layers\n",
    "# 2. output_hidden_states=True - gets activations at every layer\n",
    "# 3. PyTorch hooks - for activation patching\n",
    "# 4. model.lm_head - for decoding hidden states (Logit Lens)\n",
    "\n",
    "tl_model = None  # We'll use manual approach\n",
    "print(\"\\n‚úì Manual interpretability setup complete!\")\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895869a",
   "metadata": {},
   "source": [
    "### Validate Hooks: Can we access internal activations?\n",
    "\n",
    "This is the critical test. If VRAM is insufficient, we'll use manual hooks on the quantized model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "331754d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using manual activation extraction from quantized model...\n",
      "Model architecture: qwen3\n",
      "Number of layers: 36\n",
      "\n",
      "‚úì Manual Hook Validation:\n",
      "  ‚Ä¢ Hidden states accessible: 37 tensors\n",
      "  ‚Ä¢ (First is embeddings, rest are layer outputs)\n",
      "  ‚Ä¢ Shape: torch.Size([1, 8, 2560])\n",
      "  ‚Ä¢ Input has 8 tokens\n",
      "\n",
      "Model's next token prediction: ' '\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "test_text = \"The acceleration due to gravity on Earth is\"\n",
    "\n",
    "if tl_model is not None:\n",
    "    # Use TransformerLens if it loaded successfully\n",
    "    logits, cache = tl_model.run_with_cache(test_text)\n",
    "    \n",
    "    print(\"‚úì Hook Validation Results:\")\n",
    "    print(f\"  ‚Ä¢ Cached {len(cache)} activation types\")\n",
    "    print(f\"  ‚Ä¢ Layers: {tl_model.cfg.n_layers}\")\n",
    "    print(f\"  ‚Ä¢ Residual stream shape: {cache['resid_pre', 0].shape}\")\n",
    "    print(f\"  ‚Ä¢ Attention outputs accessible: {('attn_out', 0) in cache}\")\n",
    "    print(f\"  ‚Ä¢ MLP outputs accessible: {('mlp_out', 0) in cache}\")\n",
    "    print(f\"\\n‚úì All hooks working correctly!\")\n",
    "    \n",
    "    predicted_token_id = logits[0, -1].argmax()\n",
    "    predicted_token = tl_model.tokenizer.decode(predicted_token_id)\n",
    "    print(f\"\\nModel's next token prediction: '{predicted_token}'\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: Use manual hooks on the quantized model\n",
    "    print(\"Using manual activation extraction from quantized model...\")\n",
    "    print(f\"Model architecture: {model.config.model_type}\")\n",
    "    print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "    \n",
    "    # Run inference\n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    print(f\"\\n‚úì Manual Hook Validation:\")\n",
    "    print(f\"  ‚Ä¢ Hidden states accessible: {len(outputs.hidden_states)} tensors\")\n",
    "    print(f\"  ‚Ä¢ (First is embeddings, rest are layer outputs)\")\n",
    "    print(f\"  ‚Ä¢ Shape: {outputs.hidden_states[0].shape}\")\n",
    "    print(f\"  ‚Ä¢ Input has {inputs.input_ids.shape[1]} tokens\")\n",
    "    \n",
    "    predicted_token_id = outputs.logits[0, -1].argmax()\n",
    "    predicted_token = tokenizer.decode(predicted_token_id)\n",
    "    print(f\"\\nModel's next token prediction: '{predicted_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb4775",
   "metadata": {},
   "source": [
    "### Manual Hook Validation\n",
    "\n",
    "Testing that we can extract and interpret hidden states from all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4012a9",
   "metadata": {},
   "source": [
    "### Phase 0 Complete ‚úì\n",
    "\n",
    "**Manual Interpretability Setup**\n",
    "\n",
    "You now have the core tools validated:\n",
    "- ‚úì **Hidden State Extraction**: `output_hidden_states=True` captures all 36 layers\n",
    "- ‚úì **Logit Lens**: `model.lm_head(model.model.norm(hidden_state))` decodes any layer\n",
    "- ‚úì **Activation Patching**: PyTorch hooks for causal interventions\n",
    "- ‚úì **VRAM Efficient**: Single 4-bit model (~3GB VRAM)\n",
    "\n",
    "Ready for Phase 1: Testing CoT faithfulness with false beliefs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
